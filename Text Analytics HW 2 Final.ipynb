{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics HW2\n",
    "Billy Yuan, Nikita Lakhotia, Stuti Madaan, Tyler Nicholas, Wenduo Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "If you wish to run this notebook, please make sure the following files are in the same folder as this notebook:\n",
    "\n",
    "* [Word weights for positive and negative words](https://github.com/hitesh915/sentimentstrength/blob/master/wordwithStrength.txt)\n",
    "\n",
    "* [Yelp data](https://github.com/billy-yuan/MSBA/blob/master/Fall%202016/Text%20Analytics/HW2/yelp.csv)\n",
    "\n",
    "Credit to the following websites for their articles on sentiment analysis:\n",
    "* [Mining Twitter Data with Python Part 6 - Sentiment Analysis Basics by Marco Bonzanini](https://marcobonzanini.com/2015/05/17/mining-twitter-data-with-python-part-6-sentiment-analysis-basics/)\n",
    "* [Hitesh Parmer's repo on text mining, which contained a text file with the weights of positive and negative words](https://github.com/hitesh915/sentimentstrength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math, time, re, functools\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk as nltk\n",
    "from nltk import pos_tag, word_tokenize, classify, bigrams, trigrams\n",
    "from nltk.corpus import stopwords as stpwds\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from patsy import dmatrices\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize a lemmatizer just in case it will be used\n",
    "lmtz = WordNetLemmatizer().lemmatize\n",
    "regex_tok = RegexpTokenizer(r\"[\\w]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readData(portion, random_state=time.time()):\n",
    "    '''Read in a certain portion of data in a random manner'''\n",
    "    rd.seed(random_state)\n",
    "    skip = rd.sample(xrange(1, 19999), int(math.ceil(19999*(1-portion))))\n",
    "    data = pd.read_csv(\"yelp.csv\", skiprows=skip)\n",
    "    data[\"target\"]=data.stars.map(lambda v: 1 if v>3 else 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateTrainTest(data, portion, random_state=time.time()):\n",
    "    '''Split train and test data set'''\n",
    "    rd.seed(random_state)\n",
    "    train_index = rd.sample(xrange(len(data)), int(math.ceil(len(data)*portion)))\n",
    "    test_index = list(set(xrange(len(data)))-set(train_index))\n",
    "    train_data = data.ix[train_index]\n",
    "    test_data = data.ix[test_index]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateFormula(data):\n",
    "    '''A helper function to generate formula for regression'''\n",
    "    formula = \"target~0\"\n",
    "    for var in data.columns.values.tolist():\n",
    "        if data[var].dtype in [\"int64\", \"float64\"] and var not in [\"stars\", \"target\", \"wc\", \"Review\", \"prediction\"]:\n",
    "                \n",
    "            formula += \"+\"+var\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "    return formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitXY(data):\n",
    "    '''Split independent and dependent variables, and return X as DataFrame Y as Series'''\n",
    "    Y, X = dmatrices(generateFormula(data), data=data, return_type=\"dataframe\")\n",
    "    return X, np.ravel(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_model(X, y):\n",
    "    '''A wrapper to generate and fit a logistic regression model'''\n",
    "    model = LogisticRegression(random_state=128)\n",
    "    model.fit(X, y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of positive and negative words with their sentiment scores\n",
    "# Text file and helper function can be found at\n",
    "# https://github.com/hitesh915/sentimentstrength/blob/master/wordwithStrength.txt\n",
    "\n",
    "sentimentData = 'wordwithStrength.txt' \n",
    "\n",
    "def sentiment_dict(sentimentData=sentimentData):\n",
    "    ''' (file) -> dictionary\n",
    "    This method should take your sentiment file\n",
    "    and create a dictionary in the form {word: value}\n",
    "    '''\n",
    "    afinnfile = open(sentimentData)\n",
    "    scores = {} # initialize an empty dictionary\n",
    "    for line in afinnfile:\n",
    "        term, score = line.split(\"\\t\") # The file is tab-delimited. \"\\t\" means \"tab character\"\n",
    "        scores[term] = float(score) # Convert the score to an integer.\n",
    "\n",
    "    return scores # Print every (term, score) pair in the dictionary\n",
    "\n",
    "sentiment_values = sentiment_dict()\n",
    "senti_tuples = [(key,sentiment_values[key]) for key in sentiment_values]\n",
    "senti_tuples = sorted(senti_tuples, key=lambda tup: tup[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printAccuracy(prediction, target):\n",
    "    '''Calculate and format accuracy of prediction against target'''\n",
    "    print \"Accuracy: {:>6.4f}\".format((prediction == target).mean())\n",
    "    print \"------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review2wc(text, lem=False):\n",
    "    '''Decompose a review into tokens, removing stopwords and optionally do lemmatization'''\n",
    "    wc = {}\n",
    "    text = text.lower()\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    stopwords = stpwds.words(\"english\")\n",
    "    if lem:\n",
    "        lmtzi = lmtz\n",
    "        tokens = map(lmtz, tokens)\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "        \n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "            try:\n",
    "                wc[token] =+ 1\n",
    "            except KeyError:\n",
    "                wc[token] = 1\n",
    "    return wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateTrainTest(data, portion, random_state=time.time()):\n",
    "    '''Split train and test data set'''\n",
    "    rd.seed(random_state)\n",
    "    train_index = rd.sample(xrange(len(data)), int(math.ceil(len(data)*portion)))\n",
    "    test_index = list(set(xrange(len(data)))-set(train_index))\n",
    "    train_data = data.ix[train_index]\n",
    "    test_data = data.ix[test_index]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_prob(corpus, subset):\n",
    "    '''Given a corpus and a subset, calculate the probability of each word\n",
    "    from the corpus appearing in the subset'''\n",
    "    prob_dict = {}\n",
    "    N = sum([i for (_, i) in list(corpus.items())])\n",
    "    for key in corpus:\n",
    "        if key not in subset:\n",
    "            prob_dict[key] = 1.0 / N\n",
    "        else:\n",
    "            prob_dict[key] = subset[key] + 1.0 / N\n",
    "    return prob_dict\n",
    "\n",
    "def log_prob(term_prob_high, term_prob_low):\n",
    "    '''Given 2 subsets, calculate log relative probability o\n",
    "    a word appearing in subset 1 against in subset 2'''\n",
    "    term_log_prob = {}\n",
    "    log = math.log\n",
    "    for key in term_prob_high:\n",
    "        term_log_prob[key] = log(term_prob_high[key]/term_prob_low[key])\n",
    "    return term_log_prob\n",
    "\n",
    "def token_count(wc):\n",
    "    '''Given a list of dictionaries in the form of \"word:count\",\n",
    "    aggregate word:count in to a single dictionary'''\n",
    "    tc = {}\n",
    "    for dic in wc.tolist():\n",
    "        if len(dic) == 0: continue\n",
    "        for token, count in dic.items():\n",
    "            try:\n",
    "                tc[token] += count\n",
    "            except KeyError:\n",
    "                tc[token] = 1\n",
    "                \n",
    "    return tc\n",
    "\n",
    "def totalscore(wc, prior, benchmark):\n",
    "    '''Given a dictionary in the form of \"word:count\", \n",
    "    and reference dictionary in the form of \"word:log relative probability\",\n",
    "    calculate the sum of count*log relative probability,\n",
    "    and at the end add a prior.'''\n",
    "    prob = 0\n",
    "    for word, count in wc.items():\n",
    "        try:\n",
    "            prob += count * benchmark[word]\n",
    "        except KeyError:\n",
    "            prob += 0\n",
    "    prob += math.log(prior/(1-prior+0.00001))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NBClassifier(object):\n",
    "    '''A Naive Bayes classifier object with methods to fit on training data and \n",
    "    predict on test data'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.term_log_prob = None\n",
    "        self.prior = None\n",
    "    \n",
    "    def fit(self, data, x_label, y_label):\n",
    "        '''The core of this method is to keep a dictionary of \"word:log relative probability\"'''\n",
    "        self.X = data[x_label]\n",
    "        self.y = data[y_label]\n",
    "        self.x_label = x_label\n",
    "        self.y_label = y_label\n",
    "        token_count_total = token_count(data[x_label])\n",
    "        token_count_high = token_count(data[data[y_label]==1][x_label])\n",
    "        token_count_low = token_count(data[data[y_label]==0][x_label])\n",
    "        term_prob_high = term_prob(token_count_total, token_count_high)\n",
    "        term_prob_low = term_prob(token_count_total, token_count_low)\n",
    "        self.term_log_prob = log_prob(term_prob_high, term_prob_low)\n",
    "        self.prior = len(data[data[y_label]==1])*1.0/len(data)\n",
    "        \n",
    "    def predict(self, test, threshold=None):\n",
    "        '''Prediction can be tuned by adjusting threshold.\n",
    "        If threshold is set to None, then return actual score.'''\n",
    "        totalscore_partial = functools.partial(totalscore, \n",
    "                                               prior= self.prior,\n",
    "                                               benchmark=self.term_log_prob)\n",
    "        score = test[self.x_label].map(totalscore_partial)\n",
    "        if threshold == None:\n",
    "            return score\n",
    "        else:\n",
    "            prediction = score.map(lambda x: 1 if x>threshold else 0)\n",
    "            return prediction\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = stpwds.words('english')\n",
    "for word in stopwords[:len(stopwords)]:\n",
    "    stopwords.append(word.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentimentData = 'wordwithStrength.txt' # enter path of sentiment weights\n",
    "\n",
    "\n",
    "def sentiment_dict(sentimentData=sentimentData):\n",
    "    ''' (file) -> dictionary\n",
    "    This method should take your sentiment file\n",
    "    and create a dictionary in the form {word: value}\n",
    "    '''\n",
    "    afinnfile = open(sentimentData)\n",
    "    scores = {} # initialize an empty dictionary\n",
    "    for line in afinnfile:\n",
    "        term, score = line.split(\"\\t\") # The file is tab-delimited. \"\\t\" means \"tab character\"\n",
    "        scores[term] = float(score) # Convert the score to an integer.\n",
    "\n",
    "    return scores # Print every (term, score) pair in the dictionary\n",
    "\n",
    "sentiment_values = sentiment_dict()\n",
    "    \n",
    "senti_tuples = [(key,sentiment_values[key]) for key in sentiment_values]\n",
    "senti_tuples = sorted(senti_tuples, key=lambda tup: tup[1], reverse=False)\n",
    "\n",
    "class semanticOrientation(object):\n",
    "    '''\n",
    "    Calculates Semantic Orientations of bigrams from a list of reviews and performs binary classification.\n",
    "    \n",
    "    Note: Before running semanticOrientation(), you need to store the sentiment weights in a list of tuples\n",
    "    called senti_tuples.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.so_scores = None\n",
    "        self.predict_results = None\n",
    "        self.so_counts = None\n",
    "        self.valid_bigrams = None\n",
    "        self.word_counts = None # co-occurence matrix\n",
    "        self.tokenized = None\n",
    "        self.all_words = None\n",
    "        self.com_dict = None\n",
    "        self.bigrams_list_sep = None\n",
    "        self.bigrams_list_filtered = None\n",
    "        self.num_positive = 0\n",
    "        self.num_negative = 0\n",
    "        self.accuracy = 0\n",
    "        self.yhat = 0\n",
    "        self.negative_words = None\n",
    "        self.test_review_stop = None\n",
    "        \n",
    "\n",
    "    \n",
    "    def create_bigrams(self, reviews_, n): # n is how many words in front of and behind bigram you want\n",
    "        \n",
    "        stopwords = stpwds.words('english')\n",
    "        for word in stopwords[:len(stopwords)]:\n",
    "            stopwords.append(word.title())\n",
    "            \n",
    "        valid_pos_bigrams = [('JJ','NN'),('JJ','NNS'),('RB','JJ'),('RBR','JJ'),('RBS','JJ'),('JJ','JJ'),\n",
    "                    ('NN','JJ'),('NNS','JJ'),('RB','VB')]\n",
    "        \n",
    "        test_review_token = [regex_tok.tokenize(reviews[i].decode('utf-8')) for i in xrange(len(reviews_))]\n",
    "        test_review_stop = []\n",
    "        \n",
    "        for review in test_review_token:\n",
    "            rev_store_1 = []\n",
    "            for word in review:\n",
    "                if word not in stopwords:\n",
    "                    rev_store_1.append(word)\n",
    "                else:\n",
    "                    continue\n",
    "            test_review_stop.append(rev_store_1)\n",
    "                    \n",
    "        \n",
    "        \n",
    "        bigrams_list_sep = [list(bigrams(item)) for item in test_review_stop]\n",
    "        \n",
    "        self.bigrams_list_sep = bigrams_list_sep\n",
    "        self.tokenized = test_review_stop \n",
    "        \n",
    "        all_words = []\n",
    "\n",
    "        for rev in test_review_stop:\n",
    "            all_words += rev\n",
    "            \n",
    "        self.all_words = all_words\n",
    "        \n",
    "        # Create list of positive and negative words\n",
    "        negative_words = [senti_tuples[i][0] for i in xrange(len(senti_tuples)) if senti_tuples[i][1] < .1]\n",
    "        positive_words = [senti_tuples[i][0] for i in xrange(len(senti_tuples)) if senti_tuples[i][1] > -0.1]\n",
    "        self.negative_words = negative_words\n",
    "        ## Word counts - positive and negative\n",
    "        num_positive = len([word for word in all_words if word in positive_words])\n",
    "        num_negative = len([word for word in all_words if word in negative_words])\n",
    "        \n",
    "        self.num_positive = num_positive\n",
    "        self.num_negative = num_negative\n",
    "        \n",
    "        # Create list of stopwords\n",
    "        stopwords = stpwds.words('english')\n",
    "        \n",
    "\n",
    "        for word in stopwords[:len(stopwords)]:\n",
    "            stopwords.append(word.title())\n",
    "\n",
    "        for word in stopwords[:len(positive_words)]:\n",
    "            positive_words.append(word.title())\n",
    "\n",
    "        for word in stopwords[:len(negative_words)]:\n",
    "            negative_words.append(word.title())                  \n",
    "\n",
    "        # Filter list of bigrams of each review using Turney's rules\n",
    "        bigrams_list_filtered = []\n",
    "\n",
    "        for review in bigrams_list_sep:\n",
    "            review_store = []\n",
    "            for bigram in review:\n",
    "                tup_store = []\n",
    "                for tup in pos_tag(bigram):\n",
    "                    tup_store.append(tup[1])\n",
    "                if (tup_store[0][:2],tup_store[1][:2]) in valid_pos_bigrams:\n",
    "                    review_store.append(bigram)\n",
    "            bigrams_list_filtered.append(review_store)\n",
    "            \n",
    "        self.bigrams_list_filtered = bigrams_list_filtered\n",
    "        bigrams_list_filtered_all = [bigram for review in bigrams_list_filtered for bigram in review]\n",
    "        \n",
    "#         # list in which every item is a bigram. Will be bigrams for all articles. Only used to calculate number of\n",
    "#         # positive and negative words in corpus; these are our \"priors\" and will be used to calculate semantic orientation\n",
    "#         bigrams_list_all = [] \n",
    "\n",
    "#         for i in xrange(len(bigrams_list_filtered)):\n",
    "#             bigrams_list_all += bigrams_list_sep[i]\n",
    "\n",
    "        ## Create co-occurrence dictionary for every word\n",
    "\n",
    "        com_dict_test = {}\n",
    "\n",
    "        for review in bigrams_list_sep:\n",
    "            for i in range(len(review)):\n",
    "                com_dict_test[review[i]] = {}\n",
    "\n",
    "        for count in range(1,n+1):\n",
    "            for review in bigrams_list_sep:\n",
    "                for i in range(len(review)-count):\n",
    "                    j = i+count\n",
    "                    if review[i] in com_dict_test and review[j] in com_dict_test[review[i]]:\n",
    "                        com_dict_test[review[i]][review[j][1]] += 1\n",
    "                    elif review[i] in com_dict_test and review[j] not in com_dict_test[review[i]]:\n",
    "                        com_dict_test[review[i]][review[j][1]] = 1\n",
    "                    else: \n",
    "                        com_dict_test[review[i]] = {}\n",
    "                        com_dict_test[review[i]][review[j][1]] = 1\n",
    "\n",
    "            for review in bigrams_list_sep:\n",
    "                for i in range(count,len(review)):\n",
    "                    j = i-count\n",
    "                    if review[j][1] not in com_dict_test[review[i]]:\n",
    "                        com_dict_test[review[i]][review[j][1]] = 1\n",
    "                    else:\n",
    "                        com_dict_test[review[i]][review[j][1]] += 1 \n",
    "\n",
    "\n",
    "        com_dict_2 = {}\n",
    "        \n",
    "\n",
    "        for key in com_dict_test:\n",
    "            if key in bigrams_list_filtered_all:\n",
    "                com_dict_2[key] = com_dict_test[key]    \n",
    "        self.com_dict = com_dict_2       \n",
    "        # Calculate semantic orientations\n",
    "        \n",
    "        semantic_o_2 = {} # key is bigram. value is list (number of positive nearby, number of negative nearby, number of neutral)\n",
    "\n",
    "        for key in com_dict_2:\n",
    "            semantic_o_2[key] = [0,0,0]\n",
    "            for value in com_dict_2[key]:\n",
    "                if value in positive_words:\n",
    "                    semantic_o_2[key][0] += 1\n",
    "                elif value in negative_words:\n",
    "                    semantic_o_2[key][1] += 1\n",
    "                else:\n",
    "                    semantic_o_2[key][2] += 1\n",
    "        self.so_counts = semantic_o_2\n",
    "        \n",
    "    # Make predictions. Target should be list or series of y-values (1 or 0 for review)\n",
    "    def predict(self, target):\n",
    "        x = 0\n",
    "        so_predict = [] # predicted values using SO\n",
    "        \n",
    "        for index in range(len(self.bigrams_list_sep)):\n",
    "            rate_so_review = [bigram for bigram in self.bigrams_list_sep[index] if bigram in self.com_dict]\n",
    "\n",
    "            positive_bg = []\n",
    "            negative_bg = []\n",
    "            so_scores = []\n",
    "            \n",
    "            \n",
    "            \n",
    "            for bigram in rate_so_review:\n",
    "                if bigram in self.com_dict:\n",
    "                    positive_bg.append(self.so_counts[bigram][0])\n",
    "                else:\n",
    "                    continue\n",
    "            for bigram in rate_so_review:\n",
    "                if bigram in rate_so_review:\n",
    "                    negative_bg.append(self.so_counts[bigram][1])  \n",
    "                else:\n",
    "                    continue\n",
    "# \n",
    "            # make tuples (bigram, # positive, # negative, SO score)\n",
    "            for i in range(len(rate_so_review)):\n",
    "                if positive_bg[i] > 0 or negative_bg[i] > 0:\n",
    "                    numerator = float(positive_bg[i]) * self.num_negative\n",
    "                    denominator = float(negative_bg[i]) * self.num_positive\n",
    "                    so = math.log((numerator+.001)/ (denominator + .001))\n",
    "                    tup_store = (rate_so_review[i], positive_bg[i], negative_bg[i], so)\n",
    "\n",
    "                    so_scores.append(tup_store)\n",
    "\n",
    "\n",
    "        #     Calculate SO score\n",
    "            mean_store = []\n",
    "            \n",
    "            for i in range(len(so_scores)):\n",
    "                mean_store.append(so_scores[i][3])\n",
    "\n",
    "            if len(mean_store) != 0:\n",
    "                mean_so = sum(mean_store)/len(mean_store)\n",
    "            else:\n",
    "                mean_so = 0\n",
    "\n",
    "            if mean_so > 0:\n",
    "                so_predict.append(1)\n",
    "            else:\n",
    "                so_predict.append(0)\n",
    "        self.yhat = so_predict\n",
    "        \n",
    "#         self.predict_results = dict(Counter(so_predict))\n",
    "        self.accuracy = sum(target[i] == so_predict[i] for i in range(len(target))) / float(len(target))\n",
    "        \n",
    "    \n",
    "    def scores(self,n,desc=True):\n",
    "        \n",
    "        semantic_scores = []\n",
    "    \n",
    "        for key in self.so_counts:\n",
    "            numerator = float(self.so_counts[key][0]) * self.num_negative\n",
    "            denominator = float(self.so_counts[key][1] * self.num_positive)\n",
    "            semantic_scores.append((key, math.log(numerator + .01 / (denominator + .001))))\n",
    "        self.so_scores = semantic_scores\n",
    "        \n",
    "        return sorted(semantic_scores, key=lambda tup: tup[1], reverse=desc)[:n]\n",
    "#         return self.so_scores\n",
    "\n",
    "# x = semanticOrientation()\n",
    "# x.create_bigrams(reviews,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A\n",
    "_Ignore the text (reviews) and run a classification model with the numeric data (you can use\n",
    "standard methods like logistic regression, k-nearest neighbors or anything else). What is the best\n",
    "accuracy of your model with numeric data?_\n",
    "\n",
    "##### Brief discussion:\n",
    "\n",
    "Ignoring the text, we used a logistic regression and our best accuracy of the test set was 68.6%. We split the data set as 70% training and 30% test.\n",
    "\n",
    "The logistic regression model prediction accuracy is close to the baseline accuracy of predicting all 1 (i.e. high) in both the training and test sets.\n",
    "\n",
    "##### Analyzing the coefficients\n",
    "\n",
    "```\n",
    "Beta Coefficients \n",
    "\n",
    "Vietnamese: 0.700\n",
    "Expensive: 0.634\n",
    "votes_cool: 0.594\n",
    "Mediterranean: 0.506\n",
    "Mexican: 0.466\n",
    "Indian: 0.381\n",
    "Greek: 0.356\n",
    "French: 0.354\n",
    "Thai: 0.344\n",
    "Italian: 0.233\n",
    "Japanese: 0.214\n",
    "American: 0.192\n",
    "Others: 0.130\n",
    "VeryExpensive: 0.123\n",
    "Chinese: 0.049\n",
    "Moderate: -0.001\n",
    "Cheap: -0.234\n",
    "votes_funny: -0.243\n",
    "votes_useful: -0.265\n",
    "```\n",
    "\n",
    "The coefficients also tell an interesting story about the most important features. The top 3 coefficients were Vietnamese, Expensive, and votes_cool. \n",
    "\n",
    "The high coefficient of Vietnamese cuisine suggests that holding all other features constant, Vietnamese cuisine is a strong predictor of a highly rated restaurant. However, we weren't sure why the coefficient was so high; we hypothesized that a disproportionate percentage of reviews for Vietnamese restaurants were 4 or 5 stars. However, the percentage of reviews that were 4 or 5 stars for Vietnamese restaurants wasn't higher than that for other cuisines. \n",
    "\n",
    "\"Expensive\" was the next highest coefficient, suggesting that regardless of cuisine, expensive restaurants tend to have higher ratings. However, restaurants that were \"Very Expensive\", \"Moderate\" and \"Cheap\" had a significantly lower coefficient. It is difficult to determine why because the lines separating \"Moderate\" and \"Expensive\" are blurred. Only 297 of the 3,999 restaurants were \"Expensive\" while 1,528 were \"Moderate.\" Among the \"Expensive\" restaurants, 223 were rated 4 or 5 stars, while 799 of the \"Moderate\" restaurants were 3 stars or below.\n",
    "\n",
    "Finally, among the 3 types of votes, a review that was \"cool\" was the strongest predictor of a highly rated restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of >3 star restaurants in data: 0.6737\n",
      "------------------------------------------\n",
      "Percentage of >3 star restaurants in test: 0.6622\n",
      "------------------------------------------\n",
      "Accuracy: 0.6706\n",
      "------------------------------------------\n",
      "Beta Coefficients \n",
      "\n",
      "Vietnamese: 0.700\n",
      "Expensive: 0.634\n",
      "votes_cool: 0.594\n",
      "Mediterranean: 0.506\n",
      "Mexican: 0.466\n",
      "Indian: 0.381\n",
      "Greek: 0.356\n",
      "French: 0.354\n",
      "Thai: 0.344\n",
      "Italian: 0.233\n",
      "Japanese: 0.214\n",
      "American: 0.192\n",
      "Others: 0.130\n",
      "VeryExpensive: 0.123\n",
      "Chinese: 0.049\n",
      "Moderate: -0.001\n",
      "Cheap: -0.234\n",
      "votes_funny: -0.243\n",
      "votes_useful: -0.265\n"
     ]
    }
   ],
   "source": [
    "# readData will automatically create the target column with >3 star restaurants labeled by 1, else 0\n",
    "data = readData(0.2, random_state=8)\n",
    "print \"Percentage of >3 star restaurants in data: {:>6.4f}\".format(data.target.mean())\n",
    "print \"------------------------------------------\"\n",
    "train_1, test_1 = generateTrainTest(data, 0.7, random_state=8)\n",
    "# splitXY will automatically select all numeric columns, except stars, with target as y\n",
    "X_train, y_train = splitXY(train_1)\n",
    "model_1 = logistic_model(X_train, y_train)\n",
    "X_test, y_test = splitXY(test_1)\n",
    "print \"Percentage of >3 star restaurants in test: {:>6.4f}\".format(y_test.mean())\n",
    "print \"------------------------------------------\"\n",
    "prediction = model_1.predict(X_test)\n",
    "printAccuracy(prediction, y_test)\n",
    "\n",
    "coefficients = []\n",
    "\n",
    "len(data.columns.values)-2\n",
    "len([feature for item in model_1.coef_ for feature in item])\n",
    "\n",
    "features = X_train.columns.values\n",
    "coef = model_1.coef_[0]\n",
    "\n",
    "coef_list = []\n",
    "\n",
    "for index in range(len(features)):\n",
    "    coef_list.append((features[index], coef[index]))\n",
    "coef_list = sorted(coef_list, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "print \"Beta Coefficients\", \"\\n\"\n",
    "for tup in coef_list:\n",
    "    print \"{}: {:.3f}\".format(tup[0], tup[1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B\n",
    "_Perform a supervised classification on a subset of the corpus using the reviews only. You can\n",
    "write your code in Python or R. What accuracy do you get from this text mining exercise?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach Outline\n",
    "1. Break down Review into {word: count} dictionary, but don't create word count matrix;\n",
    "2. Train a Naive Bayes classifier (customized above) based on word: count and target label.\n",
    "\n",
    "##### Result\n",
    "The Multinomial Naive Bayes classifier had a test accuracy of 68.2%, compared with the baseline of 66.2% in the test set.\n",
    "\n",
    "##### Brief discussion\n",
    "The prediction accuracy of Naive Bayes classification is dubiously higher than of the previous logistic regression. The improvement varies when the sample is redrawn randomly from 0.01 to >.20. Though highly volatile, the increase is positive in all cases that have been tried, which means predicting high/low rating based on Review is more effective than based on the given numeric attributes. However the reviews themselves are highly variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of >3 star restaurants in test: 0.6622\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Decompose the review into a {word: count} dictionary\n",
    "data[\"wc\"] = data.Review.map(review2wc)\n",
    "# Split the train and test sets\n",
    "train_2, test_2 = generateTrainTest(data, 0.7, random_state=8) # Worry not, same random_state same rows\n",
    "print \"Percentage of >3 star restaurants in test: {:>6.4f}\".format(test_2.target.mean())\n",
    "print \"------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6822\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the customized Naive Bayes classifier\n",
    "classifier = NBClassifier()\n",
    "# Training undergoing\n",
    "classifier.fit(train_2, \"wc\", \"target\")\n",
    "prediction = classifier.predict(test_2, threshold=3.5) # Threshold set by heuristics\n",
    "printAccuracy(prediction, test_2.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C\n",
    "_Combine the numeric data and the text classification model (in task B) to create a “hybrid”\n",
    "model. It is your task to figure out how to do this. Now run this hybrid classification model and compare\n",
    "the results with those in A and B. Does the numeric data add to the predictive power relative to text?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach Outline\n",
    "1. Use NB classifier to return a *total_score* of each review, which is basically the log probability of its words\n",
    "2. Create a new column with the *total_score*, and include this column in the logistic regression model.\n",
    "\n",
    "##### Result\n",
    "The test accuracy of our hybrid model was 68.2%.\n",
    "\n",
    "##### Brief discussion\n",
    "Given that we have introduced more information, we expected that the result should have been more accurate than that from the Naive Bayes model. However, because we didn't create interaction terms, any advantage from adding information from the review may have been lost. In addition, possible co-linearity between the log probability from the Naive Bayes and other numeric attributes may have been high, and may have minimized the value that we expected the log-probability score from our Naive Bayes model to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6822\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data[\"total_score\"] = classifier.predict(data, threshold=3.5)\n",
    "train_3, test_3 = generateTrainTest(data, 0.7, random_state=8)\n",
    "X_train, y_train = splitXY(train_3)\n",
    "model_2 = logistic_model(X_train, y_train)\n",
    "X_test, y_test = splitXY(test_3)\n",
    "prediction = model_2.predict(X_test)\n",
    "printAccuracy(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task D\n",
    "_Use unsupervised sentiment analysis on the reviews (with SentiStrength or any other tool) and\n",
    "use the sentiment scores to predict high/low rating. Compare and contrast the results of tasks B and D.\n",
    "What can you conclude from your analysis?_\n",
    "\n",
    "##### Approach Outline\n",
    "1. Run each review through Sentistrength and export the results as a .txt file\n",
    "2. Read the .txt file in Python and calculate the prediction looking at the sign of the difference between positive and negative scores; a positive difference means 4 or 5 stars, and a negative difference means 3 or fewer stars.\n",
    "\n",
    "##### Results\n",
    "Using the entire data set, our model using Sentistrength had an accuracy of 58.47%.\n",
    "\n",
    "##### Brief Discussion\n",
    "Among our models using text and sentiment analysis (i.e. Parts C through E), the model using Sentistrength produced the lowest accuracy. Sentistrength is rather naive in its approach because unparsed reviews containing both positive and negative words would cancel each other out. On the contrary, our Naive Bayes model was slightly less naive than the Sentistrength model, and calculated posteriors based on the frequency of a word given a particular class, which turns out to be more accurate than Sentistrength's method, which just sums up sentiment scores with no other filters. \n",
    "\n",
    "To make the model less naive, we could also have tried parsing a review into phrases, and running each phrase through Sentistrength. Our next model in Part E uses PMI calculations to create a Semantic Orientation score based on words in proximity, which is a less naive way of doing this unsupervised learning task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5847\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_table(\"Results_SentiStrength.txt\", header = 0)\n",
    "data_full = pd.read_csv(\"yelp.csv\")\n",
    "data_full[\"target\"]=data_full.stars.map(lambda v: 1 if v>3 else 0)\n",
    "\n",
    "results['Sentiment'] = results['Positive']+results['Negative']\n",
    "\n",
    "def Prediction(Score):\n",
    "    if int(Score) >0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "results['Prediction'] = results['Sentiment'].apply(Prediction)\n",
    "senti_strength_predict = results['Prediction'].tolist()\n",
    "actual_data = data_full['target'].tolist()\n",
    "\n",
    "count = 0\n",
    "for i in range(len(senti_strength_predict)):\n",
    "    if senti_strength_predict[i] == actual_data[i]:\n",
    "        count +=1\n",
    "correct = float(count)/len(senti_strength_predict)\n",
    "print \"Accuracy: {:.4f}\".format(correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task E\n",
    "\n",
    "_Implement the PMI approach to sentiment analysis (in either Python or R), and run the\n",
    "classification model with the sentiment scores. How do your results compare with those in Task D?_\n",
    "\n",
    "##### Approach Outline\n",
    "1. Filter bigrams according to modified POS rules from Turney's article\n",
    "2. Calculate Semantic Orientation of each bigram using the number of positive and negative words within 3 words (excluding stopwords) of the bigram.\n",
    "3. For each review, take the average of the Semantic Orientation scores of each bigram. If the average Semantic Orientation is greater than 1, then the review is positive. Otherwise, it's negative.\n",
    "\n",
    "We used the Turney article as our basis and chose to use Semantic Orientation, which is just the difference in PMI's between a bigram near a positive word and a bigram near a negative word. Our Semantic Orientation calculation was:\n",
    "$$ SO_{bigram} = log(\\frac {n_{+,bigram} \\times N_{negative} + .01}{n_{-, bigram} \\times N_{positive} + .01}) $$\n",
    "\n",
    "To start, we filtered the bigrams by using simplified part-of-speech rules derived from the Turney article. The only simplification we made was that we did not care what the POS of the word after the bigram was. \n",
    "\n",
    "\n",
    "$N_{negative}$ and $N_{positive}$ are the total number of positive and negative words in the entire corpus of reviews; they serve as our \"priors.\" We checked whether a  word in the corpus was in a list of positive and negative words (the link to the document can be found at the top of the notebook). The counts of words in the corpus that were in the \"master\" list of positive and negative words served as our values for $N_{negative}$ and $N_{positive}$. In the first 1000 reviews:\n",
    "* $ N_{negative} = 10,483 $\n",
    "\n",
    "* $ N_{positive} = 16,874 $\n",
    "\n",
    "$n_{+,bigram}$ and $n_{-,bigram}$ are the counts of positive and negative words within a given number of words from the bigram (excluding stopwords). We chose to look at words within 3 words of the bigram. For example, in the bigram ('fresh warm'), $n_{+,bigram}$ is 2: 'always' and 'helps' are positive words, and $n_{-,bigrams}$ is 0. \n",
    "```\n",
    "(u'always', u'helps'): {u'always': 1,\n",
    "                        u'excellent': 1,\n",
    "                        u'food': 1,\n",
    "                        u'hot': 1,\n",
    "                        u'service': 1,\n",
    "                        u'waitress': 1}\n",
    "```\n",
    "\n",
    "To classify a review, we took the average of the Semantic Orientations of every bigram in the review.\n",
    "\n",
    "$$ SO_{i} =  \\frac{\\sum_{i=1}^{R} SO_{i j}}{R}$$\n",
    "where $SO_{i}$ is the Semantic Orientation of review $i$, $R$ is the number of bigrams (filtered using Turney's POS rules) in the review and $j$ is the index of each bigram in the review.\n",
    "\n",
    "$$Class_{i} = \\begin{cases} high, &\\mbox{if }SO_{i} > 0 \\\\ low, & \\mbox{if }SO_{i} < 0 \\end {cases}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Results\n",
    "\n",
    "Testing the first 1000 reviews, our model had an accuracy score of 60.7%, which is better than the accuracy using the Senti-Strength model.\n",
    "\n",
    "##### Brief Discussion\n",
    "\n",
    "Depending on the sample of reviews we chose, the accuracy of our model using PMI / Semantic Orientation ranged between 60-70%. We can inspect the top 10 bigrams with the highest and lowest Semantic Orientation.\n",
    "\n",
    "```\n",
    "Top 10 Lowest Semantic Orientations (First 1000 Reviews)\n",
    "\n",
    " ((u'little', u'corner'), -15.576037341956656),\n",
    " ((u'less', u'richness'), -15.576037341956656),\n",
    " ((u'felt', u'sick'), -15.576037341956656),\n",
    " ((u'sweet', u'Phoenix'), -15.352893794941304),\n",
    " ((u'entirely', u'sweet'), -15.352893794941304),\n",
    " ((u'sad', u'salad'), -15.352893794941304),\n",
    " ((u'sick', u'days'), -15.352893794941304),\n",
    " ((u'appetite', u'things'), -15.352893794941304),\n",
    " ((u'flat', u'bread'), -15.352893794941304),\n",
    " ((u'horrible', u'see'), -15.352893794941304)\n",
    "\n",
    "Top 10 Highest Semantic Orientations (First 1000 Reviews)\n",
    "\n",
    " ((u'happy', u'hour'), 12.934340800634144),\n",
    " ((u'really', u'good'), 12.290790561213711),\n",
    " ((u'Mexican', u'food'), 12.25800073839094),\n",
    " ((u'Indian', u'food'), 12.224099186715321),\n",
    " ((u'first', u'time'), 11.947845810087092),\n",
    " ((u'food', u'great'), 11.901325794452529),\n",
    " ((u'pretty', u'good'), 11.901325794452077),\n",
    " ((u'light', u'rail'), 11.747175114625065),\n",
    " ((u'service', u'great'), 11.690016700785623),\n",
    " ((u'great', u'service'), 11.690016700785494)\n",
    "```\n",
    "\n",
    "The words with the highest Semantic Orientations all seem to make sense. However, some of the bigrams with the lowest Semantic Orientations are a bit harder to interpret. \n",
    "\n",
    "For example, why does \"flat bread\" have such a low semantic orientation? We can inspect the words that were close to this bigram in our sample of 1000 reviews. The keys are the words near the bigram and the values are how many times a word appeared within 3 non-stopwords to a bigram.\n",
    "\n",
    "```\n",
    "{u'baked': 1, u'dark': 1, u'flat': 1, u'house': 1, u'smoke': 1, u'wood': 1}\n",
    "```\n",
    "It turns out that the weights in the text file for all of the words except for \"house\"  are negative; in this case, $n_{-,bigram}$ is 5 and $n_{+,bigram}$ is 0, which explains why the Semantic Orientation is negative.\n",
    "\n",
    "To further improve the model, we could inspect the sentiment weights of the words that shouldn't be negative and change them. In this case, it's unclear why \"smoke\" and \"baked\" should have negative sentiment weights. Ideally, whether a word is positive or negative should be based on the context. In this case, the context is restaurants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Part E: 0.607000\n"
     ]
    }
   ],
   "source": [
    "data_raw = pd.read_csv(\"yelp.csv\")[:1000]\n",
    "    \n",
    "data_raw['target'] = data_raw['stars'].map(lambda t: 1 if t > 3 else 0)\n",
    "reviews = data_raw['Review']\n",
    "target_e = data_raw['target']\n",
    "\n",
    "part_e_model = semanticOrientation()\n",
    "part_e_model.create_bigrams(reviews,3) # find words 3 words behind and 3 words ahead of bigram (excluding stop words)\n",
    "part_e_model.predict(target_e)\n",
    "print \"Accuracy - Part E: {:4f}\".format(part_e_model.accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task F\n",
    "\n",
    "_What are the top 5 “attributes” of a restaurant that are associated with (i) high and (ii) low\n",
    "ratings? That is, when people rate a restaurant high or low, are they more likely to mention service,\n",
    "ambiance, etc.?_\n",
    "\n",
    "##### Approach Outline\n",
    "1. For restaurants with High rating, tokenize the reviews.\n",
    "2. Get the POS tag for unigrams in each of the reviews.\n",
    "3. From the above POS corpus, filter words corresponding to POS=\"NN\" i.e. Extracting all the Nouns from the Reviews\n",
    "4. Calculate the freqeuncy of words and diplay it in descending order to find the most occuring Nouns specific to High Ratings.\n",
    "5. Repeat the same for Low Rating restaurants.\n",
    "\n",
    "##### Results\n",
    "```\n",
    "Words with Highest Relative Probabilities for High Ratings (285,443)\n",
    "\n",
    "    (u'food', 10075 - 3.5%),\n",
    "    (u'place', 9335 - 3.3%),\n",
    "    (u'time', 4261 - 1.5%),\n",
    "    (u'service', 4011 - 1.4%),\n",
    "    (u'menu', 3319 - 1.2%),\n",
    " \n",
    "Words with Highest Relative Probabilities for Low Ratings (164,184)\n",
    "\n",
    "    (u'food', 6335 - 3.8%),\n",
    "    (u'place', 4709 - 2.9%),\n",
    "    (u'service', 2462 - 1.5%),\n",
    "    (u'time', 2443 - 1.5%),\n",
    "    (u'restaurant', 1914 - 1.2%)\n",
    "```\n",
    "##### Brief Discussion\n",
    "For both high and low rated reviews, attributes for \"food\", \"place\", \"service\" and \"time\" were important to customers. We expected that the rankings or proportions of attributes would be different between reviews of high and low rated restaurants, but it turns out that regardless of a restaurant's rating, people mention the same attributes.\n",
    "\n",
    "\"Menu\" and \"restaurant\" are the only words that are different in the top-5 rankings of attributes for high and low rated restaurants. One potential hypothesis is that people who give a low rating are bad-mouthing the restaurant more, and not talking about the menu as much. However, both \"menu\" and \"restaurant\" are in the top-10 list of most commonly used attributs for both high and low rated restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_all = pd.read_csv(\"yelp.csv\")\n",
    "    \n",
    "data_all['target'] = data_all['stars'].map(lambda t: 1 if t > 3 else 0)\n",
    "reviews = data_all['Review']\n",
    "\n",
    "pos=[]\n",
    "unigram=[]\n",
    "\n",
    "high_rated= data_all[data_all['target']==1] ## reviews with high rating\n",
    "low_rated= data_all[data_all['target']==0] ## reviews with low rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "pattern = r'\\s?(\\w+)\\s?'\n",
    "\n",
    "def freq_attribute(data):\n",
    "    pos=[]\n",
    "    unigram=[]\n",
    "    lst_final=list(data.ix[:,'Review']) ## extracting reviews only\n",
    "    for i in range(len(lst_final)): \n",
    "        new_list = lst_final[i] ## \n",
    "        text = nltk.regexp_tokenize(new_list.decode('utf-8'), pattern) ## tokenizing the review \n",
    "        unigram.append(text) \n",
    "    unigrams2=unigram\n",
    "    pos2=[]\n",
    "    for i in unigrams2:\n",
    "        pos2.append(nltk.pos_tag(i)) ## getting pos for each word\n",
    "    merged = list(itertools.chain(*pos2))\n",
    "    a = [item[0] for item in merged if item[1] == 'NN'] ## filtering for nouns i.e. POS=\"NN\"\n",
    "    return Counter(a).most_common() ## fetching frequency of Noun and displaying in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_words = freq_attribute(high_rated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'food', 10075)\n",
      "(u'place', 9335)\n",
      "(u'time', 4261)\n",
      "(u'service', 4011)\n",
      "(u's', 4004)\n",
      "(u'menu', 3319)\n",
      "(u'restaurant', 3289)\n",
      "(u'order', 2437)\n",
      "(u'lunch', 2426)\n",
      "(u't', 2225)\n"
     ]
    }
   ],
   "source": [
    "for tup in high_words[:10]:\n",
    "    print tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "low_words = freq_attribute(low_rated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'food', 6335)\n",
      "(u'place', 4709)\n",
      "(u'service', 2462)\n",
      "(u'time', 2443)\n",
      "(u's', 2137)\n",
      "(u't', 1978)\n",
      "(u'restaurant', 1914)\n",
      "(u'order', 1798)\n",
      "(u'menu', 1719)\n",
      "(u'lunch', 1295)\n"
     ]
    }
   ],
   "source": [
    "for tup in low_words[:10]:\n",
    "    print tup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
